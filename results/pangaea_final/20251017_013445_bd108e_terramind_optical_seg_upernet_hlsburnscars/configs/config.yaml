task:
  trainer:
    _target_: pangaea.engine.trainer.SegTrainer
    model: null
    train_loader: null
    optimizer: null
    lr_scheduler: null
    evaluator: null
    exp_dir: null
    device: null
    criterion: null
    n_epochs: 80
    precision: fp32
    ckpt_interval: 20
    eval_interval: 5
    log_interval: 5
    best_metric_key: mIoU
    use_wandb: ${use_wandb}
  evaluator:
    _target_: pangaea.engine.evaluator.SegEvaluator
    val_loader: null
    exp_dir: null
    device: null
    use_wandb: ${use_wandb}
    inference_mode: sliding
    sliding_inference_batch: 8
dataset:
  _target_: pangaea.datasets.hlsburnscars.HLSBurnScars
  dataset_name: HLSBurnScars
  root_path: ./data/hlsburnscars
  download_url: https://huggingface.co/datasets/ibm-nasa-geospatial/hls_burn_scars/resolve/main/hls_burn_scars.tar.gz?download=true
  auto_download: true
  img_size: 512
  multi_temporal: false
  multi_modal: false
  ignore_index: -1
  num_classes: 2
  classes:
  - Not burned
  - Burn scar
  distribution:
  - 0.88889
  - 0.11111
  bands:
    optical:
    - B2
    - B3
    - B4
    - B8A
    - B11
    - B12
  data_mean:
    optical:
    - 0.033349706741586264
    - 0.05701185520536176
    - 0.05889748132001316
    - 0.2323245113436119
    - 0.1972854853760658
    - 0.11944914225186566
  data_std:
    optical:
    - 0.02269135568823774
    - 0.026807560223070237
    - 0.04004109844362779
    - 0.07791732423672691
    - 0.08708738838140137
    - 0.07241979477437814
  data_min:
    optical:
    - 0.0
    - 0.0
    - 0.0
    - 0.0
    - 0.0
    - 0.0
  data_max:
    optical:
    - 0.0
    - 0.0
    - 0.0
    - 0.0
    - 0.0
    - 0.0
encoder:
  _target_: pangaea.encoders.terramind_encoder.terramind_v1_large
  encoder_weights: C:\Users\Dhenenjay\Axion-Sat\weights\hf\TerraMind-1.0-large\TerraMind_v1_large.pt
  download_url: null
  input_size: 224
  patch_size: 16
  merge_method: mean
  modalities:
  - S2L2A
  input_bands:
    optical:
    - B1
    - B2
    - B3
    - B4
    - B5
    - B6
    - B7
    - B8
    - B8A
    - B10
    - B11
    - B12
  output_layers:
  - 7
  - 11
  - 15
  - 23
  output_dim: 1024
decoder:
  _target_: pangaea.decoders.upernet.SegUPerNet
  channels: 512
  encoder: null
  num_classes: ${dataset.num_classes}
  finetune: ${finetune}
preprocessing:
  train:
    _target_: pangaea.engine.data_preprocessor.Preprocessor
    preprocessor_cfg:
    - _target_: pangaea.engine.data_preprocessor.RandomCropToEncoder
    - _target_: pangaea.engine.data_preprocessor.BandFilter
    - _target_: pangaea.engine.data_preprocessor.NormalizeMeanStd
    - _target_: pangaea.engine.data_preprocessor.BandPadding
  val:
    _target_: pangaea.engine.data_preprocessor.Preprocessor
    preprocessor_cfg:
    - _target_: pangaea.engine.data_preprocessor.BandFilter
    - _target_: pangaea.engine.data_preprocessor.NormalizeMeanStd
    - _target_: pangaea.engine.data_preprocessor.BandPadding
  test:
    _target_: pangaea.engine.data_preprocessor.Preprocessor
    preprocessor_cfg:
    - _target_: pangaea.engine.data_preprocessor.BandFilter
    - _target_: pangaea.engine.data_preprocessor.NormalizeMeanStd
    - _target_: pangaea.engine.data_preprocessor.BandPadding
criterion:
  _target_: torch.nn.CrossEntropyLoss
  ignore_index: ${dataset.ignore_index}
lr_scheduler:
  _target_: pangaea.utils.schedulers.MultiStepLR
  optimizer: null
  total_iters: null
  lr_milestones:
  - 0.6
  - 0.9
optimizer:
  _target_: torch.optim.AdamW
  params: null
  lr: 0.0001
  betas:
  - 0.9
  - 0.999
  weight_decay: 0.05
train: true
work_dir: ../results/pangaea_final
seed: 234
use_wandb: false
wandb_run_id: null
num_workers: 0
batch_size: 8
test_num_workers: 0
test_batch_size: 1
finetune: false
ckpt_dir: null
limited_label_train: 1
limited_label_val: 1
limited_label_strategy: stratified
stratification_bins: 3
data_replicate: 1
use_final_ckpt: false
